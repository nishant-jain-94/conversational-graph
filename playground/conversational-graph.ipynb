{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "from flask import Flask, g, Response, request\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import SequentialBackoffTagger\n",
    "from neo4j.v1 import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__, static_url_path='/static/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Connecting to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "db = driver.session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PartsOfQueryTagger(SequentialBackoffTagger):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        SequentialBackoffTagger.__init__(self, *args, **kwargs)\n",
    "    \n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        results = db.run('MATCH (n {name: {name}}) return n', {\"name\":tokens[index]})\n",
    "        return results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = PartsOfQueryTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_words = tagger.tag(word_tokenize(\"create a concept node with name as MEANStack\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Record n=<Node id=2013 labels={'intent', 'action'} properties={'name': 'create'}>>\n",
      "<Record n=<Node id=1984 labels={'node'} properties={'name': 'concept'}>>\n",
      "<Record n=<Node id=1981 labels={'node'} properties={'name': 'node'}>>\n",
      "<Record n=<Node id=1998 labels={'property'} properties={'name': 'name'}>>\n"
     ]
    }
   ],
   "source": [
    "for word, results in tagged_words:\n",
    "    for record in results:\n",
    "        print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(sentence):\n",
    "    \"\"\"\n",
    "    Extracts features from a sentence\n",
    "    \n",
    "    sentence: Inputs a sentence in natural language\n",
    "    Returns: a dict of feature, indicating a presence or absence of certain features\n",
    "    \"\"\"\n",
    "    tokenized_words = [word.lower() for word in word_tokenize(sentence)]\n",
    "    features = ['node', 'nodes', 'relations', 'relationships', 'csv', 'build', 'make', 'match', 'find', 'fetch', 'create', 'get', 'number', 'count', 'relation']\n",
    "    word_dict = {}\n",
    "    for feature in features:\n",
    "        if feature in tokenized_words:\n",
    "            word_dict[feature] = +1\n",
    "        else:\n",
    "            word_dict[feature] = -1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "datasets = ['createNodes', 'createRelations', 'getCountOfNodes', 'getNodes', 'importNodesFromFile', 'importRelationsFromFile']\n",
    "raw_dataset = []\n",
    "for file in datasets:\n",
    "    with open('data/'+file) as f:\n",
    "        raw_dataset += [(extract_features(question), file) for question in f]\n",
    "random.shuffle(raw_dataset)\n",
    "size = int(len(raw_dataset) * 0.5)\n",
    "train_set, test_set = raw_dataset[:size], raw_dataset[size:]\n",
    "queryClassifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@app.route('/classify', methods=['POST'])\n",
    "def classify_query():\n",
    "    print(request.get_json());\n",
    "    return queryClassifier.classify(extract_features(sentence));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify_query('create relations from csv file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(port=8081)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
